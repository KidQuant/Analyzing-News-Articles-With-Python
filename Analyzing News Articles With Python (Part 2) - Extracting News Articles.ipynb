{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing News Articles With Python (Part 2)\n",
    "\n",
    "*By Andre Sealy (Updated 6/21/2020)*\n",
    "\n",
    "This is the second part of a multi-part series in analyzing news articles using Python. Part One involves using Python and the All Sides website to extract the most important articles based on the stories that are interesting to us. In this part, we will extract the most important information for analyzing news articles, namely the authors and the article body. To do this, we will use an API called **News-Please**.\n",
    "\n",
    "### News Please\n",
    "\n",
    "[News-Please](https://github.com/fhamborg/news-please) is an open source news crawler that extracts structured information from almost any website. You can use it to follow recursively internal hyperlinks and read RSS feeds to extract most recent and old archived articles.\n",
    "\n",
    "#### Importing the Modules\n",
    "\n",
    "Let's begin to load the necessary modules for the project, which includes the following:\n",
    "\n",
    "* pandas\n",
    "* csv\n",
    "* pickle\n",
    "* matplotlib\n",
    "* difflib\n",
    "* itertools\n",
    "* newsplease\n",
    "\n",
    "We have used `pandas` and `csv` before, so if you've completed the first part, then you're already familiar with how it's used. This tutorial will be utilizing some newer modules that will help us extract articles.\n",
    "\n",
    "We will be using the `pickle`, which will allow us to take a complex object structure and transform it into a stream of bytes that can be saved onto the hard drive through the process of *serialization*. We won't get into what this process is here. If you want to learn more, [here is a good resource](https://docs.python-guide.org/scenarios/serialization/).\n",
    "\n",
    "We will specifically be using the `difflib.SequenceMatcher` method to compare pairs of input sequences. It's basically a way of looking at matching sequences without all of the \"junk\" involved ([here](https://towardsdatascience.com/sequencematcher-in-python-6b1e6f3915fc) is a resource on how it all works, for further learning).\n",
    "\n",
    "Finally, we will be using the `newsplease` module, which will allow us to extract information from the article links that we have collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from itertools import accumulate, chain, repeat, tee\n",
    "import difflib\n",
    "import matplotlib\n",
    "from newsplease import NewsPlease"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to reintroduce the `link_file.txt` that we created in Part One. We're going to use this to create a list containing strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.washingtonexaminer.com/opinion/just-imagine-if-trump-had-stopped-immigration-in-early-february',\n",
       " 'https://www.washingtonpost.com/immigration/coronavirus-trump-immigration/2020/04/21/a2a465aa-837a-11ea-9728-c74380d9d410_story.html',\n",
       " 'https://www.reuters.com/article/us-health-coronavirus-usa/u-s-coronavirus-response-deepens-divide-as-trump-suspends-immigration-idUSKCN2231TU',\n",
       " 'https://www.foxnews.com/politics/trump-suspend-immigration-executive-order-coronavirus',\n",
       " 'https://www.foxnews.com/politics/court-hands-trump-win-in-sanctuary-city-grant']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = r'Insight/news-lens'\n",
    "\n",
    "with open(r'{}/link_file.txt'.format(filepath)) as f:\n",
    "    news_links = [line.replace(\"\\n\", \"\") for line in f]\n",
    "    \n",
    "news_links[5:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are going to compile the list of URLs into chunks. To do this, we're going to create a function that accepts two arguments: the list of URLs and the number of chunks that we want to create.\n",
    "\n",
    "Assuming that the number of chucks is greater than zero, the function is going to measure the size of the list and divide it by the number of chunks we are trying to create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['https://www.washingtonpost.com/immigration/coronavirus-trump-immigration/2020/04/21/a2a465aa-837a-11ea-9728-c74380d9d410_story.html',\n",
       "  'https://www.reuters.com/article/us-health-coronavirus-usa/u-s-coronavirus-response-deepens-divide-as-trump-suspends-immigration-idUSKCN2231TU'],\n",
       " ['https://www.foxnews.com/politics/trump-suspend-immigration-executive-order-coronavirus',\n",
       "  'https://www.foxnews.com/politics/court-hands-trump-win-in-sanctuary-city-grant'],\n",
       " ['https://www.axios.com/court-trump-immigration-sanctuary-cities-565ee05c-46ea-4894-8e1b-52f6a0fde7c8.html',\n",
       "  'https://www.thedailybeast.com/trump-administration-can-withhold-grants-from-sanctuary-cities-court-rules'],\n",
       " ['https://www.nytimes.com/2020/02/13/us/politics/border-wall-funds-pentagon.html',\n",
       "  'https://www.washingtonexaminer.com/policy/defense-national-security/trumps-plan-to-strip-planes-ships-and-vehicles-from-pentagon-budget-to-fund-border-wall-draws-bipartisan-howls-of-protest-from-congress']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chuck(xs, n):\n",
    "    assert n > 0\n",
    "    L = len(xs)\n",
    "    s, r = divmod(L, n)\n",
    "    widths = chain(repeat(s+1, r), repeat(s, n-r))\n",
    "    offsets = accumulate(chain((0,), widths))\n",
    "    b, e = tee(offsets)\n",
    "    next(e)\n",
    "    return [xs[s] for s in map(slice, b, e)]\n",
    "\n",
    "\n",
    "batch = chuck(news_links, 251)\n",
    "\n",
    "batch[3:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of the chunks is to make it more difficult for the News Please API to run into errors. So now that we have created the chunks, we are going to use the `NewsPlease` module to extract information from the URLs.\n",
    "\n",
    "We will get this information using the `NewsPlease.from_urls` method, which extracts all of the important information from each article. (Source, Description, Authors, etc.)\n",
    "\n",
    "Once we extract the information, we will serialize this information using a pickle rather than storing the information in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-59:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\andre\\Anaconda3\\lib\\threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\andre\\Anaconda3\\lib\\threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\andre\\Anaconda3\\lib\\site-packages\\newsplease\\crawler\\simple_crawler.py\", line 31, in _fetch_url\n",
      "    html = urllib.request.urlopen(req, data=None, timeout=timeout).read()\n",
      "  File \"C:\\Users\\andre\\Anaconda3\\lib\\urllib\\request.py\", line 222, in urlopen\n",
      "    return opener.open(url, data, timeout)\n",
      "  File \"C:\\Users\\andre\\Anaconda3\\lib\\urllib\\request.py\", line 531, in open\n",
      "    response = meth(req, response)\n",
      "  File \"C:\\Users\\andre\\Anaconda3\\lib\\urllib\\request.py\", line 641, in http_response\n",
      "    'http', request, response, code, msg, hdrs)\n",
      "  File \"C:\\Users\\andre\\Anaconda3\\lib\\urllib\\request.py\", line 569, in error\n",
      "    return self._call_chain(*args)\n",
      "  File \"C:\\Users\\andre\\Anaconda3\\lib\\urllib\\request.py\", line 503, in _call_chain\n",
      "    result = func(*args)\n",
      "  File \"C:\\Users\\andre\\Anaconda3\\lib\\urllib\\request.py\", line 649, in http_error_default\n",
      "    raise HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "urllib.error.HTTPError: HTTP Error 404: Not Found\n",
      "\n",
      "error getting summary: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\andre\\Anaconda3\\lib\\site-packages\\readability\\readability.py\", line 189, in summary\n",
      "    self._html(True)\n",
      "  File \"C:\\Users\\andre\\Anaconda3\\lib\\site-packages\\readability\\readability.py\", line 132, in _html\n",
      "    self.html = self._parse(self.input)\n",
      "  File \"C:\\Users\\andre\\Anaconda3\\lib\\site-packages\\readability\\readability.py\", line 141, in _parse\n",
      "    doc, self.encoding = build_doc(input)\n",
      "  File \"C:\\Users\\andre\\Anaconda3\\lib\\site-packages\\readability\\htmls.py\", line 17, in build_doc\n",
      "    encoding = get_encoding(page) or 'utf-8'\n",
      "  File \"C:\\Users\\andre\\Anaconda3\\lib\\site-packages\\readability\\encoding.py\", line 46, in get_encoding\n",
      "    page.decode(encoding)\n",
      "LookupError: unknown encoding: ><script type=\n"
     ]
    }
   ],
   "source": [
    "def article_crawler():\n",
    "    # crawler\n",
    "    n = 0\n",
    "    for i in range(0, len(batch)):\n",
    "        try:\n",
    "            slice = batch[i]\n",
    "            # print slice\n",
    "            slice_name = str(i) + '-NewsPlease-articleCrawl.p'\n",
    "            article_information = NewsPlease.from_urls(slice)\n",
    "            pickle.dump(article_information, open(slice_name, 'wb'))\n",
    "            n += 1\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "article_crawler()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed that the program has run into errors. This is perfectly normal, as some slugs for the URLs have changed, or some pages may have been deleted. We want an idea of how many articles our program has managed to extract. \n",
    "\n",
    "The following code chunk will look through each of the articles and attempt to extract relevant information. After its finished, it will calculate the success rate. A high success rate suggests that we were able to extract the majority of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The extraction process yielded 291 articles, or 97.32% of the total.\n"
     ]
    }
   ],
   "source": [
    "# helper function\n",
    "def make_unique(url_list):\n",
    "   # Not order preserving    \n",
    "   unique = set(url_list)\n",
    "   return list(unique)\n",
    "\n",
    "def check_data(filepath):\n",
    "    scraped = []\n",
    "    not_scraped = []\n",
    "\n",
    "    for i in range(0, 251):\n",
    "        try:\n",
    "            file_path = filepath+\"/crawl/\"\n",
    "            open_crawl = pickle.load(open(file_path + str(i)\n",
    "                                          + \"-NewsPlease-articleCrawl.p\", \"rb\"))\n",
    "            for url in open_crawl:\n",
    "                text = open_crawl[str(url)].maintext\n",
    "                if text == None:\n",
    "                    not_scraped.append(url)\n",
    "                else:\n",
    "                    scraped.append(url)\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "\n",
    "    scraped = make_unique(scraped)\n",
    "    return scraped, not_scraped\n",
    "            \n",
    "success, fail = check_data(filepath)\n",
    "\n",
    "# analyze data collection success\n",
    "def percentage(part, whole):\n",
    "    percent = 100 * float(part)/float(whole)\n",
    "    format = \"{0:.2f}\".format(percent)\n",
    "    return format+'%'\n",
    "\n",
    "print(\"The extraction process yielded \" \n",
    "      + str(len(success)) + \" articles, or \"\n",
    "      + percentage(len(success),len(news_links)) \n",
    "      + \" of the total.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were able to extract 97.32% of the total amount of articles -- more than enough for our analysis. Now we want to get additional information that the All Sides website was not able to provide directly: The Title, Authors, and Source.\n",
    "\n",
    "The process is simple. We will loop through each pickle in the directory. The `pickle.load` method will open a News Please object, which will be used to extract the title, authors, and source.\n",
    "\n",
    "The source provides the source domain (.com, .gov. .edu, etc.), which is not exactly what we want. We created a loop that replaces parts of the domain with blank spaces.\n",
    "\n",
    "Once everything is done, the object will be returned in the form of a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(filepath):\n",
    "    news_dict = {}\n",
    "\n",
    "    remove_list = ['www.', 'www1.' '.com', '.gov', '.org', 'beta.', ',eu',\n",
    "                   '.co.uk', 'europe', 'gma', 'blogs', 'in.', 'm.',\n",
    "                   'eclipse2017.', 'money', 'insider', 'news.', 'finance.'\n",
    "                   'www1.']\n",
    "\n",
    "    for i in range(0, 220):\n",
    "        try:\n",
    "            file_path = filepath + \"/crawl/\"\n",
    "            open_crawl = pickle.load(open(file_path+str(i)\n",
    "                                          + \"-NewsPlease-articleCrawl.p\", \"rb\"))\n",
    "\n",
    "            for url in open_crawl:\n",
    "                text = open_crawl[str(url)].maintext\n",
    "                if text != None:\n",
    "                    title = open_crawl[str(url)].title\n",
    "                    authors = ', '.join(open_crawl[str(url)].authors)\n",
    "                    source = open_crawl[str(url)].source_domain\n",
    "\n",
    "                    for seq in remove_list:\n",
    "                        if seq in source:\n",
    "                            source = source.replace(seq, \"\")\n",
    "\n",
    "                    date = open_crawl[str(url)].date_publish\n",
    "\n",
    "                    news_dict[str(url)] = [source, title, authors, date, text]\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "\n",
    "    return news_dict\n",
    "\n",
    "\n",
    "all_news = get_data(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the function returns an object named `all_news`, which provides the title, authors and source domain. Now we want an idea of how many articles we've extracted from each source. The following functions will count the number of articles for each source and return the results in the form of a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'buzzfeedcom': 3,\n",
       " 'reuters.com': 20,\n",
       " 'dailycaller.com': 4,\n",
       " 'af.reuters.com': 1,\n",
       " 'cnn.com': 12,\n",
       " 'washingtonexaminer.com': 10,\n",
       " 'washingtonpost.com': 11,\n",
       " 'foxcom': 25,\n",
       " 'thedailybeast.com': 2,\n",
       " 'axios.com': 3,\n",
       " 'nytimes.com': 13,\n",
       " 'apcom': 7,\n",
       " 'bbc.com': 6,\n",
       " 'aljazeera.com': 2,\n",
       " 'washingtontimes.com': 17,\n",
       " 'npr': 5,\n",
       " 'nationalreview.com': 9,\n",
       " 'vox.com': 7,\n",
       " 'usatoday.com': 12,\n",
       " 'nbccom': 4,\n",
       " 'bloomberg.com': 3,\n",
       " 'wsj.com': 17,\n",
       " 'jacobinmag.com': 1,\n",
       " 'cbscom': 3,\n",
       " 'thehill.com': 15,\n",
       " 'dailymail': 3,\n",
       " 'slate.com': 1,\n",
       " 'thefederalist.com': 1,\n",
       " 'msnbc.com': 1,\n",
       " 'huffpost.com': 1,\n",
       " 'breitbart.com': 3,\n",
       " 'theblaze.com': 2,\n",
       " 'newsmax.com': 1,\n",
       " 'www1.cbn.com': 2,\n",
       " 'rollcall.com': 1,\n",
       " 'townhall.com': 8,\n",
       " 'politico.com': 7,\n",
       " 'nypost.com': 3,\n",
       " 'splintercom': 1,\n",
       " 'huffingtonpost.com': 8,\n",
       " 'latimes.com': 3,\n",
       " 'vanityfair.com': 1,\n",
       " 'salon.com': 1,\n",
       " 'theguardian.com': 2}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_sources(all_news):\n",
    "    news_sources = {}\n",
    "    for article in all_news:\n",
    "        source = all_news[article][0]\n",
    "        if source not in news_sources:\n",
    "            news_sources[source] = 1\n",
    "        else:\n",
    "            news_sources[source] += 1\n",
    "    return news_sources\n",
    "\n",
    "check_sources = get_sources(all_news)\n",
    "\n",
    "check_sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have extracted the articles, but it's not exactly presented in a way that we would like. Unforunately, we would need to clean this up manually. (This is not necessary for those who are fine with how the data turned out)\n",
    "\n",
    "Next, we want to assign the bias rating to all of the sources we have extracted. For that, we will import the All Sides Media Bias rating, which is simply a compliation of all ratings on their website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_sources = {'Buzzfeed': 3, 'Reuters': 20, 'daily caller': 4, 'af.reuters': 1, 'CNN (Web News)': 12, \n",
    " 'The Washington Examiner': 10, 'The Washington Post': 11, 'Fox News': 25, 'the daily beast': 2,\n",
    " 'axios': 3, 'New York Times': 13, 'AP': 7, 'bbc': 6, 'Al Jazeera': 2, 'Washington Times': 17,\n",
    " 'NPR News': 5, 'National Review': 9, 'Vox': 7, 'USA TODAY': 12, 'NBCNews.com': 4, 'Bloomberg': 3,\n",
    " 'The Wall Street Journal': 17, 'Jacobin Magazine': 1, 'CBS News': 3, 'The Hill': 15, 'daily mail': 3,\n",
    " 'slate': 1, 'the fFederalist': 1, 'MSNBC': 1, 'HuffPost': 1, 'breitbart': 3, 'the blaze': 2,\n",
    " 'NewMax': 1, 'cbn': 2, 'rollcall': 1, 'Townhall': 8, 'politico': 7, 'New York Post': 3, 'splinter news': 1,\n",
    " 'The Huffington Post': 8, 'Los Angeles Times': 3, 'Vanity Fair': 1, 'Salon': 1, 'The Guardian': 2}\n",
    "\n",
    "bias_ratings = r\"{}/allsides-media-bias-ratings.csv\".format(filepath)\n",
    "\n",
    "bias_dict = {}\n",
    "\n",
    "with open(bias_ratings, mode='r') as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    bias_dict = {rows[0]: rows[1] for rows in reader}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to merge our dictionary of sources with the bias alignment from All Sides. The function will attempt to match the source created in our dictionary to the sources in the media bias document. If there is a match, it will assign it's proper alignment to the source. If the source isn't found, its alignment will be left unknown.\n",
    "\n",
    "The end result will be a data frame of all the sources we've extracted, the number of articles extracted, and the alignment of each source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>News_Source</th>\n",
       "      <th>Bias</th>\n",
       "      <th>Article_Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Fox News</td>\n",
       "      <td>Lean Right</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Reuters</td>\n",
       "      <td>Center</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Wall Street Journal- News</td>\n",
       "      <td>Center</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Washington Times</td>\n",
       "      <td>Lean Right</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>The Hill</td>\n",
       "      <td>Center</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>New York Times</td>\n",
       "      <td>Lean Left</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>USA TODAY</td>\n",
       "      <td>Center</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CNN (Web News)</td>\n",
       "      <td>Lean Left</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Washington Post</td>\n",
       "      <td>Lean Left</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Washington Examiner</td>\n",
       "      <td>Right</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  News_Source        Bias  Article_Count\n",
       "7                    Fox News  Lean Right             25\n",
       "1                     Reuters      Center             20\n",
       "21  Wall Street Journal- News      Center             17\n",
       "14           Washington Times  Lean Right             17\n",
       "24                   The Hill      Center             15\n",
       "10             New York Times   Lean Left             13\n",
       "18                  USA TODAY      Center             12\n",
       "4              CNN (Web News)   Lean Left             12\n",
       "6             Washington Post   Lean Left             11\n",
       "5         Washington Examiner       Right             10"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def string_sim(a, b):\n",
    "    seq = difflib.SequenceMatcher(None, a, b)\n",
    "    sim = seq.ratio() * 100\n",
    "    return sim\n",
    "\n",
    "\n",
    "def replace_names(check_sources, bias_dict):\n",
    "    real_source = {}\n",
    "\n",
    "    for entry in check_sources:\n",
    "        for source in bias_dict:\n",
    "\n",
    "            sim = string_sim(entry, source)\n",
    "            count = check_sources[entry]\n",
    "\n",
    "            if entry not in real_source:\n",
    "                real_source[entry] = [source, sim, count]\n",
    "\n",
    "            else:\n",
    "                if sim > real_source[entry][1]:\n",
    "                    real_source[entry] = [source, sim, count]\n",
    "\n",
    "    raw_data = {'News_Source': [], 'Bias': [], 'Article_Count': []}\n",
    "\n",
    "    for key in real_source:\n",
    "        new_key = real_source[key][0]\n",
    "        new_count = real_source[key][2]\n",
    "\n",
    "        bias = bias_dict[new_key]\n",
    "\n",
    "        raw_data['News_Source'].append(new_key)\n",
    "        raw_data['Article_Count'].append(new_count)\n",
    "        raw_data['Bias'].append(bias)\n",
    "\n",
    "    for bias_rating in raw_data['Bias']:\n",
    "        if bias_rating == 'Mixed':\n",
    "            bias_rating = bias_rating.replace('Mixed', 'Center')\n",
    "\n",
    "    source_bias = pd.DataFrame(\n",
    "        raw_data, columns=['News_Source', 'Bias', 'Article_Count'])\n",
    "\n",
    "    return source_bias\n",
    "\n",
    "\n",
    "updated_info = replace_names(check_sources, bias_dict)\n",
    "updated_info.to_csv('news-corpus-info.csv', index=False)\n",
    "updated_info.sort_values('Article_Count', ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, Fox News has the largest number of articles in our dataset, with 25 pieces. Reuters is right behind Fox News with 20 articles, followed by the Wall Street Journal (News Section), Washington Times, and The Hill with 20, 17, and 15 articles, respectively.\n",
    "\n",
    "I can only speak for myself, but from what I see so far, I believe this alignment is correct for the most part. Fox News tends to lean right, while CNN and New York Times lean more left.\n",
    "\n",
    "I think some people would disagree with the alignment of the Wall Street Journal, as the publisher is owned by News Corp, which is the parent company of Fox News. I believe most people would be inclined to agree, as far as community feedback on All Sides is concerned.\n",
    "\n",
    "![](https://kidquant.com/post/images/Analyzing-News/topics-allsides5.PNG)\n",
    "\n",
    "Still, whether or not a news outlet is owned or operated by a particular person has little to do with its overall objectivity and bias. All Sides has conducted an [in-depth analysis](https://www.allsides.com/news-source/wall-street-journal-media-bias) of major news publications such as The Wall Street Journal and has found that outlet is more aligned to the center than its peers. (Keep in mind, a Center alignment doesn't mean better!)\n",
    "\n",
    "Now that we have a better idea of the types of articles we've extracted, it's time to bring everything together. We will be importing the `allsides-content.csv` file that we created during Part One.\n",
    "\n",
    "When opening the file, we're going to include all of the contents inside the CSV in a dictionary. However, we will also be including the text of the article, as well as the name of the authors (if available).\n",
    "\n",
    "Once the extraction process is done, we're going to transform it into a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>main_headline</th>\n",
       "      <th>authors</th>\n",
       "      <th>description</th>\n",
       "      <th>source</th>\n",
       "      <th>bias</th>\n",
       "      <th>headline</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>June 22nd, 2020</td>\n",
       "      <td>b'Trump Admin Suspends Certain Visas Through E...</td>\n",
       "      <td>Hamed Aleaziz, Jeremy Singer-Vine, Adolfo Flor...</td>\n",
       "      <td>The Trump administration announced Monday that...</td>\n",
       "      <td>BuzzFeed News</td>\n",
       "      <td>Left</td>\n",
       "      <td>b'Trump Is Suspending Certain Visas For Foreig...</td>\n",
       "      <td>https://www.buzzfeednews.com/article/adolfoflo...</td>\n",
       "      <td>BuzzFeed News has reporters around the world b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>June 22nd, 2020</td>\n",
       "      <td>b'Trump Admin Suspends Certain Visas Through E...</td>\n",
       "      <td>Ted Hesson, Min Read</td>\n",
       "      <td>The Trump administration announced Monday that...</td>\n",
       "      <td>Reuters</td>\n",
       "      <td>Center</td>\n",
       "      <td>b'Trump to suspend entry of certain foreign wo...</td>\n",
       "      <td>https://www.reuters.com/article/us-usa-immigra...</td>\n",
       "      <td>WASHINGTON (Reuters) - U.S. President Donald T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>June 22nd, 2020</td>\n",
       "      <td>b'Trump Admin Suspends Certain Visas Through E...</td>\n",
       "      <td></td>\n",
       "      <td>The Trump administration announced Monday that...</td>\n",
       "      <td>The Daily Caller</td>\n",
       "      <td>Right</td>\n",
       "      <td>b'Trump To Suspend Visas Through End Of The Ye...</td>\n",
       "      <td>https://dailycaller.com/2020/06/22/exclusive-t...</td>\n",
       "      <td>President Donald Trump will sign an executive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>April 24th, 2020</td>\n",
       "      <td>b'Perspectives Trump s Immigration Executive O...</td>\n",
       "      <td>Ted Hesson, Min Read</td>\n",
       "      <td>President Donald Trump's executive order suspe...</td>\n",
       "      <td>Reuters</td>\n",
       "      <td>Center</td>\n",
       "      <td>b\"Inside Trump's proposal to suspend some lega...</td>\n",
       "      <td>https://af.reuters.com/article/worldNews/idAFK...</td>\n",
       "      <td>WASHINGTON (Reuters) - President Donald Trump ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>April 24th, 2020</td>\n",
       "      <td>b'Perspectives Trump s Immigration Executive O...</td>\n",
       "      <td>Opinion Rafia Zakaria</td>\n",
       "      <td>President Donald Trump's executive order suspe...</td>\n",
       "      <td>CNN - Editorial</td>\n",
       "      <td>Left</td>\n",
       "      <td>b\"Trump's moves on immigration reveal his true...</td>\n",
       "      <td>https://www.cnn.com/2020/04/22/opinions/trump-...</td>\n",
       "      <td>Rafia Zakaria is the author of \" The Upstairs ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               date                                      main_headline  \\\n",
       "0   June 22nd, 2020  b'Trump Admin Suspends Certain Visas Through E...   \n",
       "1   June 22nd, 2020  b'Trump Admin Suspends Certain Visas Through E...   \n",
       "2   June 22nd, 2020  b'Trump Admin Suspends Certain Visas Through E...   \n",
       "3  April 24th, 2020  b'Perspectives Trump s Immigration Executive O...   \n",
       "4  April 24th, 2020  b'Perspectives Trump s Immigration Executive O...   \n",
       "\n",
       "                                             authors  \\\n",
       "0  Hamed Aleaziz, Jeremy Singer-Vine, Adolfo Flor...   \n",
       "1                               Ted Hesson, Min Read   \n",
       "2                                                      \n",
       "3                               Ted Hesson, Min Read   \n",
       "4                              Opinion Rafia Zakaria   \n",
       "\n",
       "                                         description            source  \\\n",
       "0  The Trump administration announced Monday that...     BuzzFeed News   \n",
       "1  The Trump administration announced Monday that...           Reuters   \n",
       "2  The Trump administration announced Monday that...  The Daily Caller   \n",
       "3  President Donald Trump's executive order suspe...           Reuters   \n",
       "4  President Donald Trump's executive order suspe...   CNN - Editorial   \n",
       "\n",
       "      bias                                           headline  \\\n",
       "0    Left   b'Trump Is Suspending Certain Visas For Foreig...   \n",
       "1  Center   b'Trump to suspend entry of certain foreign wo...   \n",
       "2   Right   b'Trump To Suspend Visas Through End Of The Ye...   \n",
       "3  Center   b\"Inside Trump's proposal to suspend some lega...   \n",
       "4    Left   b\"Trump's moves on immigration reveal his true...   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.buzzfeednews.com/article/adolfoflo...   \n",
       "1  https://www.reuters.com/article/us-usa-immigra...   \n",
       "2  https://dailycaller.com/2020/06/22/exclusive-t...   \n",
       "3  https://af.reuters.com/article/worldNews/idAFK...   \n",
       "4  https://www.cnn.com/2020/04/22/opinions/trump-...   \n",
       "\n",
       "                                                text  \n",
       "0  BuzzFeed News has reporters around the world b...  \n",
       "1  WASHINGTON (Reuters) - U.S. President Donald T...  \n",
       "2  President Donald Trump will sign an executive ...  \n",
       "3  WASHINGTON (Reuters) - President Donald Trump ...  \n",
       "4  Rafia Zakaria is the author of \" The Upstairs ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def open_sesame(filepath):\n",
    "    datafile = open(r'{}/allsides-content.csv'.format(filepath),\n",
    "                    'r', encoding='utf-8')\n",
    "    myreader = csv.reader(datafile)\n",
    "    return myreader\n",
    "\n",
    "# return dict of stored values\n",
    "\n",
    "\n",
    "def fill_dict():\n",
    "    read_in = {'date': [], 'main_headline': [], 'description': [], 'source': [],\n",
    "               'bias': [], 'headline': [], 'link': []}\n",
    "\n",
    "    i = 0\n",
    "    for key in read_in:\n",
    "        myreader2 = open_sesame(filepath)\n",
    "        read_in[key] = [row[i] for row in myreader2][1:]\n",
    "        i += 1\n",
    "\n",
    "        # add info from NewsPlease crawl\n",
    "    read_in['text'] = []\n",
    "    read_in['authors'] = []\n",
    "    for url in read_in['link']:\n",
    "        try:\n",
    "            read_in['text'].append(all_news[str(url)][4])\n",
    "            read_in['authors'].append(all_news[str(url)][2])\n",
    "        except KeyError:\n",
    "            read_in['text'].append('None')\n",
    "            read_in['authors'].append('None')\n",
    "\n",
    "    return read_in\n",
    "\n",
    "\n",
    "formatted = fill_dict()\n",
    "\n",
    "df = pd.DataFrame(formatted, columns=['date', 'main_headline', 'authors', 'description',\n",
    "                                      'source', 'bias', 'headline', 'link', 'text'])\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have more complete information. However, we still want to clean it up a bit. We're going to start by dropping all of the rows with missing stories (or mixed stories). After that, we're going to drop the stories that are less than 250 characters and greater than 20,000 characters.\n",
    "\n",
    "Afterward, we'll save the results to a CSV for further analysis later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>main_headline</th>\n",
       "      <th>authors</th>\n",
       "      <th>description</th>\n",
       "      <th>source</th>\n",
       "      <th>bias</th>\n",
       "      <th>headline</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-06-22</td>\n",
       "      <td>b'Trump Admin Suspends Certain Visas Through E...</td>\n",
       "      <td>Hamed Aleaziz, Jeremy Singer-Vine, Adolfo Flor...</td>\n",
       "      <td>The Trump administration announced Monday that...</td>\n",
       "      <td>BuzzFeed News</td>\n",
       "      <td>Left</td>\n",
       "      <td>b'Trump Is Suspending Certain Visas For Foreig...</td>\n",
       "      <td>https://www.buzzfeednews.com/article/adolfoflo...</td>\n",
       "      <td>BuzzFeed News has reporters around the world b...</td>\n",
       "      <td>3358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-06-22</td>\n",
       "      <td>b'Trump Admin Suspends Certain Visas Through E...</td>\n",
       "      <td>Ted Hesson, Min Read</td>\n",
       "      <td>The Trump administration announced Monday that...</td>\n",
       "      <td>Reuters</td>\n",
       "      <td>Center</td>\n",
       "      <td>b'Trump to suspend entry of certain foreign wo...</td>\n",
       "      <td>https://www.reuters.com/article/us-usa-immigra...</td>\n",
       "      <td>WASHINGTON (Reuters) - U.S. President Donald T...</td>\n",
       "      <td>6015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-06-22</td>\n",
       "      <td>b'Trump Admin Suspends Certain Visas Through E...</td>\n",
       "      <td></td>\n",
       "      <td>The Trump administration announced Monday that...</td>\n",
       "      <td>The Daily Caller</td>\n",
       "      <td>Right</td>\n",
       "      <td>b'Trump To Suspend Visas Through End Of The Ye...</td>\n",
       "      <td>https://dailycaller.com/2020/06/22/exclusive-t...</td>\n",
       "      <td>President Donald Trump will sign an executive ...</td>\n",
       "      <td>1954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-04-24</td>\n",
       "      <td>b'Perspectives Trump s Immigration Executive O...</td>\n",
       "      <td>Ted Hesson, Min Read</td>\n",
       "      <td>President Donald Trump's executive order suspe...</td>\n",
       "      <td>Reuters</td>\n",
       "      <td>Center</td>\n",
       "      <td>b\"Inside Trump's proposal to suspend some lega...</td>\n",
       "      <td>https://af.reuters.com/article/worldNews/idAFK...</td>\n",
       "      <td>WASHINGTON (Reuters) - President Donald Trump ...</td>\n",
       "      <td>5310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-04-24</td>\n",
       "      <td>b'Perspectives Trump s Immigration Executive O...</td>\n",
       "      <td>Opinion Rafia Zakaria</td>\n",
       "      <td>President Donald Trump's executive order suspe...</td>\n",
       "      <td>CNN - Editorial</td>\n",
       "      <td>Left</td>\n",
       "      <td>b\"Trump's moves on immigration reveal his true...</td>\n",
       "      <td>https://www.cnn.com/2020/04/22/opinions/trump-...</td>\n",
       "      <td>Rafia Zakaria is the author of \" The Upstairs ...</td>\n",
       "      <td>8191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date                                      main_headline  \\\n",
       "0 2020-06-22  b'Trump Admin Suspends Certain Visas Through E...   \n",
       "1 2020-06-22  b'Trump Admin Suspends Certain Visas Through E...   \n",
       "2 2020-06-22  b'Trump Admin Suspends Certain Visas Through E...   \n",
       "3 2020-04-24  b'Perspectives Trump s Immigration Executive O...   \n",
       "4 2020-04-24  b'Perspectives Trump s Immigration Executive O...   \n",
       "\n",
       "                                             authors  \\\n",
       "0  Hamed Aleaziz, Jeremy Singer-Vine, Adolfo Flor...   \n",
       "1                               Ted Hesson, Min Read   \n",
       "2                                                      \n",
       "3                               Ted Hesson, Min Read   \n",
       "4                              Opinion Rafia Zakaria   \n",
       "\n",
       "                                         description            source  \\\n",
       "0  The Trump administration announced Monday that...     BuzzFeed News   \n",
       "1  The Trump administration announced Monday that...           Reuters   \n",
       "2  The Trump administration announced Monday that...  The Daily Caller   \n",
       "3  President Donald Trump's executive order suspe...           Reuters   \n",
       "4  President Donald Trump's executive order suspe...   CNN - Editorial   \n",
       "\n",
       "      bias                                           headline  \\\n",
       "0    Left   b'Trump Is Suspending Certain Visas For Foreig...   \n",
       "1  Center   b'Trump to suspend entry of certain foreign wo...   \n",
       "2   Right   b'Trump To Suspend Visas Through End Of The Ye...   \n",
       "3  Center   b\"Inside Trump's proposal to suspend some lega...   \n",
       "4    Left   b\"Trump's moves on immigration reveal his true...   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.buzzfeednews.com/article/adolfoflo...   \n",
       "1  https://www.reuters.com/article/us-usa-immigra...   \n",
       "2  https://dailycaller.com/2020/06/22/exclusive-t...   \n",
       "3  https://af.reuters.com/article/worldNews/idAFK...   \n",
       "4  https://www.cnn.com/2020/04/22/opinions/trump-...   \n",
       "\n",
       "                                                text  text_len  \n",
       "0  BuzzFeed News has reporters around the world b...      3358  \n",
       "1  WASHINGTON (Reuters) - U.S. President Donald T...      6015  \n",
       "2  President Donald Trump will sign an executive ...      1954  \n",
       "3  WASHINGTON (Reuters) - President Donald Trump ...      5310  \n",
       "4  Rafia Zakaria is the author of \" The Upstairs ...      8191  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(df[df.text == 'None'].index)\n",
    "df = df.drop(df[df.text == 'Mixed'].index)\n",
    "df.date = pd.to_datetime(df.date)\n",
    "\n",
    "df['text_len'] = df.apply(lambda row: len(row.text), axis=1)\n",
    "\n",
    "df = df.drop(df[df.text_len > 20000].index)\n",
    "df = df.drop(df[df.text_len < 250].index)\n",
    "\n",
    "# write to csv\n",
    "df.to_csv('news-corpus-df.csv', index=False)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bias</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Center</th>\n",
       "      <td>89.0</td>\n",
       "      <td>3435.662921</td>\n",
       "      <td>2214.848517</td>\n",
       "      <td>348.0</td>\n",
       "      <td>1968.00</td>\n",
       "      <td>3283.0</td>\n",
       "      <td>5025.00</td>\n",
       "      <td>11675.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Left</th>\n",
       "      <td>82.0</td>\n",
       "      <td>5431.207317</td>\n",
       "      <td>3648.040576</td>\n",
       "      <td>301.0</td>\n",
       "      <td>2493.75</td>\n",
       "      <td>4915.0</td>\n",
       "      <td>7931.25</td>\n",
       "      <td>19492.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Right</th>\n",
       "      <td>89.0</td>\n",
       "      <td>3831.337079</td>\n",
       "      <td>2549.135803</td>\n",
       "      <td>531.0</td>\n",
       "      <td>2153.00</td>\n",
       "      <td>3242.0</td>\n",
       "      <td>4757.00</td>\n",
       "      <td>17786.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         count         mean          std    min      25%     50%      75%  \\\n",
       "bias                                                                        \n",
       "Center    89.0  3435.662921  2214.848517  348.0  1968.00  3283.0  5025.00   \n",
       "Left      82.0  5431.207317  3648.040576  301.0  2493.75  4915.0  7931.25   \n",
       "Right     89.0  3831.337079  2549.135803  531.0  2153.00  3242.0  4757.00   \n",
       "\n",
       "             max  \n",
       "bias              \n",
       "Center   11675.0  \n",
       "Left     19492.0  \n",
       "Right    17786.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bybias = df.groupby('bias')\n",
    "bybias['text_len'].describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
